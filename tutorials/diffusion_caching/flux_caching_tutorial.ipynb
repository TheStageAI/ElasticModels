{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux Caching Tutorial: Accelerating Elastic Models with cache-dit\n",
    "\n",
    "This tutorial demonstrates how to apply **cache-dit** caching techniques to **Elastic Flux models** for faster inference.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Elastic Models** allow you to trade off quality for speed by using different model sizes (S, XL, original). \n",
    "**cache-dit** is a caching framework that reduces redundant computations during diffusion model inference.\n",
    "\n",
    "By combining these two approaches, we can achieve significant speedups while maintaining quality.\n",
    "\n",
    "### What we'll cover:\n",
    "1. Loading Elastic Flux models in different modes (S, XL, original)\n",
    "2. Applying DualCache (aggressive and conservative) caching strategies\n",
    "3. Comparing inference times with and without caching\n",
    "4. Evaluating image quality metrics\n",
    "5. Visualizing results\n",
    "\n",
    "### DualCache Strategies:\n",
    "\n",
    "**DualCache Aggressive** - High speedup, more caching:\n",
    "- `Fn=1, Bn=0` (minimal recomputation)\n",
    "- `rdt=0.2` (high threshold for cache invalidation)\n",
    "- `max_continuous_cached_steps=10` (long cache reuse)\n",
    "- Best for: Static scenes, mid-diffusion steps\n",
    "\n",
    "**DualCache Conservative** - Balanced speedup, safer caching:\n",
    "- `Fn=4, Bn=0` (more recomputation layers)\n",
    "- `rdt=0.05` (low threshold, more frequent cache refresh)\n",
    "- `max_continuous_cached_steps=3` (shorter cache reuse)\n",
    "- Best for: Dynamic scenes, correcting accumulated errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "**⚠️ IMPORTANT: Diffusers Version Compatibility**\n",
    "\n",
    "This tutorial uses `cache-dit` which is designed for newer versions of `diffusers` (with Chroma, HiDream models support). However, `elastic_models` may require an older version of `diffusers`.\n",
    "\n",
    "**Two solutions:**\n",
    "1. **Use our compatibility patch** (recommended) - see below\n",
    "2. **Manually edit cache-dit** - comment out imports in:\n",
    "   - `/path/to/cache_dit/cache_factory/patch_functors/functor_chroma.py`\n",
    "   - `/path/to/cache_dit/cache_factory/patch_functors/functor_hidream.py`\n",
    "\n",
    "Our utils automatically patch missing classes, so cache-dit works with older diffusers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add current directory to path for importing utils\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "# Import only utility functions (visualization, saving, compatibility)\n",
    "from flux_caching_tutorial_utils import (\n",
    "    setup_diffusers_compatibility,\n",
    "    set_seed,\n",
    "    visualize_comparison,\n",
    "    create_performance_charts,\n",
    "    print_performance_summary,\n",
    "    save_results,\n",
    ")\n",
    "\n",
    "# Setup compatibility patches BEFORE importing cache-dit\n",
    "setup_diffusers_compatibility()\n",
    "\n",
    "# Now import elastic models and cache-dit\n",
    "from elastic_models.diffusers import DiffusionPipeline as ElasticDiffusionPipeline\n",
    "import cache_dit\n",
    "from cache_dit import BasicCacheConfig, BlockAdapter, ForwardPattern\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device and dtype\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = 'black-forest-labs/FLUX.1-dev'\n",
    "HF_TOKEN = None  # Set your HuggingFace token here if needed\n",
    "HF_CACHE_DIR = '/mount/huggingface_cache'  # Change to your cache directory\n",
    "MODEL_PATH = None  # Path to elastic model weights (if custom)\n",
    "\n",
    "# Generation parameters\n",
    "WIDTH = 1024\n",
    "HEIGHT = 1024\n",
    "NUM_INFERENCE_STEPS = 28\n",
    "GUIDANCE_SCALE = 3.5\n",
    "SEED = 42\n",
    "\n",
    "# Test prompts\n",
    "TEST_PROMPTS = [\n",
    "    \"A majestic lion standing on a rocky cliff at sunset\",\n",
    "    \"A futuristic city skyline with flying cars and neon lights\",\n",
    "    \"A beautiful garden with blooming flowers and butterflies\",\n",
    "]\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path('./flux_caching_tutorial_results')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Image size: {WIDTH}x{HEIGHT}\")\n",
    "print(f\"Inference steps: {NUM_INFERENCE_STEPS}\")\n",
    "print(f\"Test prompts: {len(TEST_PROMPTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Functions for Elastic Models and Cache-Dit\n",
    "\n",
    "These are the key functions demonstrating how to use cache-dit with elastic_models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_elastic_pipeline(mode='original', model_path=None):\n",
    "    \"\"\"\n",
    "    Load Elastic Flux pipeline in specified mode.\n",
    "    \n",
    "    This shows how to load elastic models in different configurations:\n",
    "    - 'original': Full model\n",
    "    - 'XL': Larger elastic variant\n",
    "    - 'S': Smaller elastic variant (faster)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading Elastic Flux pipeline - Mode: {mode}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if mode == 'original':\n",
    "        pipeline = ElasticDiffusionPipeline.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=dtype,\n",
    "            cache_dir=HF_CACHE_DIR,\n",
    "            token=HF_TOKEN,\n",
    "            device_map=device,\n",
    "        )\n",
    "    else:\n",
    "        # Load elastic model with specific mode\n",
    "        pipeline = ElasticDiffusionPipeline.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=dtype,\n",
    "            cache_dir=HF_CACHE_DIR,\n",
    "            token=HF_TOKEN,\n",
    "            mode=mode,  # 'XL' or 'S'\n",
    "            device_map=device,\n",
    "            __model_path=model_path,\n",
    "        )\n",
    "    \n",
    "    pipeline = pipeline.to(device)\n",
    "    print(f\"✓ Pipeline loaded successfully\")\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def enable_dualcache(pipeline, mode='conservative'):\n",
    "    \"\"\"\n",
    "    Enable DualCache on Flux pipeline.\n",
    "    \n",
    "    This demonstrates how to apply cache-dit to elastic models:\n",
    "    \n",
    "    - 'aggressive': More caching, higher speedup\n",
    "      * Fn=1, rdt=0.2, max_continuous=10\n",
    "    - 'conservative': Safer caching, balanced speedup\n",
    "      * Fn=4, rdt=0.05, max_continuous=3\n",
    "    \"\"\"\n",
    "    print(f\"\\nEnabling DualCache ({mode} mode)...\")\n",
    "    \n",
    "    # Configure cache parameters based on mode\n",
    "    if mode == 'aggressive':\n",
    "        Fn = 1\n",
    "        Bn = 0\n",
    "        max_warmup_steps = 8\n",
    "        max_continuous_cached_steps = 10\n",
    "        residual_diff_threshold = 0.2\n",
    "    else:  # conservative\n",
    "        Fn = 4\n",
    "        Bn = 0\n",
    "        max_warmup_steps = 8\n",
    "        max_continuous_cached_steps = 3\n",
    "        residual_diff_threshold = 0.05\n",
    "    \n",
    "    # Create cache configuration\n",
    "    cache_config = BasicCacheConfig(\n",
    "        Fn_compute_blocks=Fn,\n",
    "        Bn_compute_blocks=Bn,\n",
    "        max_warmup_steps=max_warmup_steps,\n",
    "        max_cached_steps=-1,\n",
    "        max_continuous_cached_steps=max_continuous_cached_steps,\n",
    "        residual_diff_threshold=residual_diff_threshold,\n",
    "    )\n",
    "    \n",
    "    # Check if blocks are compiled (important for elastic models)\n",
    "    first_block = pipeline.transformer.transformer_blocks[0]\n",
    "    is_compiled = hasattr(first_block, '__wrapped__') or 'Compiled' in type(first_block).__name__\n",
    "    \n",
    "    # Enable cache using BlockAdapter for Flux architecture\n",
    "    cache_dit.enable_cache(\n",
    "        BlockAdapter(\n",
    "            pipe=pipeline,\n",
    "            transformer=pipeline.transformer,\n",
    "            blocks=[\n",
    "                pipeline.transformer.transformer_blocks,\n",
    "                pipeline.transformer.single_transformer_blocks,\n",
    "            ],\n",
    "            forward_pattern=[\n",
    "                ForwardPattern.Pattern_1,  # For transformer_blocks\n",
    "                ForwardPattern.Pattern_3,  # For single_transformer_blocks\n",
    "            ],\n",
    "            check_forward_pattern=not is_compiled,\n",
    "        ),\n",
    "        cache_config=cache_config\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ DualCache enabled: Fn={Fn}, rdt={residual_diff_threshold}, max_continuous={max_continuous_cached_steps}\")\n",
    "\n",
    "\n",
    "def disable_cache(pipeline):\n",
    "    \"\"\"Disable cache on pipeline.\"\"\"\n",
    "    cache_dit.disable_cache(pipeline)\n",
    "    print(\"✓ Cache disabled\")\n",
    "\n",
    "\n",
    "def generate_and_time(pipeline, prompt, seed=42):\n",
    "    \"\"\"Generate image and measure time.\"\"\"\n",
    "    set_seed(seed)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = pipeline(\n",
    "        prompt=prompt,\n",
    "        width=WIDTH,\n",
    "        height=HEIGHT,\n",
    "        num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "        guidance_scale=GUIDANCE_SCALE,\n",
    "        generator=generator,\n",
    "    )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return result.images[0], elapsed\n",
    "\n",
    "\n",
    "print(\"✓ Core functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment: Mode XL\n",
    "\n",
    "We'll compare:\n",
    "1. XL mode without caching (baseline)\n",
    "2. XL mode with DualCache Aggressive\n",
    "3. XL mode with DualCache Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xl = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT: Mode XL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load XL pipeline\n",
    "pipeline_xl = load_elastic_pipeline(mode='XL', model_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 XL - No Caching (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup\n",
    "print(\"\\nWarmup...\")\n",
    "_ = generate_and_time(pipeline_xl, TEST_PROMPTS[0], seed=SEED)\n",
    "print(\"✓ Warmup complete\")\n",
    "\n",
    "# Generate test images without caching\n",
    "print(\"\\nGenerating images (no caching)...\")\n",
    "images_xl_nocache = []\n",
    "times_xl_nocache = []\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"\\nPrompt {i+1}/{len(TEST_PROMPTS)}: {prompt}\")\n",
    "    image, elapsed = generate_and_time(pipeline_xl, prompt, seed=SEED+i)\n",
    "    images_xl_nocache.append(image)\n",
    "    times_xl_nocache.append(elapsed)\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "avg_time_xl_nocache = np.mean(times_xl_nocache)\n",
    "print(f\"\\nAverage time (XL no caching): {avg_time_xl_nocache:.2f}s\")\n",
    "\n",
    "results_xl['no_cache'] = {\n",
    "    'images': images_xl_nocache,\n",
    "    'times': times_xl_nocache,\n",
    "    'avg_time': avg_time_xl_nocache\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 XL - DualCache Aggressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable aggressive caching\n",
    "enable_dualcache(pipeline_xl, mode='aggressive')\n",
    "\n",
    "# Warmup with cache\n",
    "print(\"\\nWarmup with cache...\")\n",
    "_ = generate_and_time(pipeline_xl, TEST_PROMPTS[0], seed=SEED)\n",
    "print(\"✓ Warmup complete\")\n",
    "\n",
    "# Generate test images\n",
    "print(\"\\nGenerating images (DualCache Aggressive)...\")\n",
    "images_xl_aggressive = []\n",
    "times_xl_aggressive = []\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"\\nPrompt {i+1}/{len(TEST_PROMPTS)}: {prompt}\")\n",
    "    image, elapsed = generate_and_time(pipeline_xl, prompt, seed=SEED+i)\n",
    "    images_xl_aggressive.append(image)\n",
    "    times_xl_aggressive.append(elapsed)\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "avg_time_xl_aggressive = np.mean(times_xl_aggressive)\n",
    "speedup_xl_aggressive = avg_time_xl_nocache / avg_time_xl_aggressive\n",
    "print(f\"\\nAverage time (XL aggressive): {avg_time_xl_aggressive:.2f}s\")\n",
    "print(f\"Speedup: {speedup_xl_aggressive:.2f}x\")\n",
    "\n",
    "cache_stats_xl_aggressive = cache_dit.summary(pipeline_xl)\n",
    "\n",
    "results_xl['aggressive'] = {\n",
    "    'images': images_xl_aggressive,\n",
    "    'times': times_xl_aggressive,\n",
    "    'avg_time': avg_time_xl_aggressive,\n",
    "    'speedup': speedup_xl_aggressive,\n",
    "    'cache_stats': cache_stats_xl_aggressive\n",
    "}\n",
    "\n",
    "disable_cache(pipeline_xl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 XL - DualCache Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable conservative caching\n",
    "enable_dualcache(pipeline_xl, mode='conservative')\n",
    "\n",
    "# Warmup with cache\n",
    "print(\"\\nWarmup with cache...\")\n",
    "_ = generate_and_time(pipeline_xl, TEST_PROMPTS[0], seed=SEED)\n",
    "print(\"✓ Warmup complete\")\n",
    "\n",
    "# Generate test images\n",
    "print(\"\\nGenerating images (DualCache Conservative)...\")\n",
    "images_xl_conservative = []\n",
    "times_xl_conservative = []\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"\\nPrompt {i+1}/{len(TEST_PROMPTS)}: {prompt}\")\n",
    "    image, elapsed = generate_and_time(pipeline_xl, prompt, seed=SEED+i)\n",
    "    images_xl_conservative.append(image)\n",
    "    times_xl_conservative.append(elapsed)\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "avg_time_xl_conservative = np.mean(times_xl_conservative)\n",
    "speedup_xl_conservative = avg_time_xl_nocache / avg_time_xl_conservative\n",
    "print(f\"\\nAverage time (XL conservative): {avg_time_xl_conservative:.2f}s\")\n",
    "print(f\"Speedup: {speedup_xl_conservative:.2f}x\")\n",
    "\n",
    "cache_stats_xl_conservative = cache_dit.summary(pipeline_xl)\n",
    "\n",
    "results_xl['conservative'] = {\n",
    "    'images': images_xl_conservative,\n",
    "    'times': times_xl_conservative,\n",
    "    'avg_time': avg_time_xl_conservative,\n",
    "    'speedup': speedup_xl_conservative,\n",
    "    'cache_stats': cache_stats_xl_conservative\n",
    "}\n",
    "\n",
    "disable_cache(pipeline_xl)\n",
    "del pipeline_xl\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n✓ XL experiments complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment: Mode S\n",
    "\n",
    "We'll compare:\n",
    "1. S mode without caching (baseline)\n",
    "2. S mode with DualCache Aggressive\n",
    "3. S mode with DualCache Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_s = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT: Mode S\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load S pipeline\n",
    "pipeline_s = load_elastic_pipeline(mode='S', model_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 S - No Caching (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup\n",
    "print(\"\\nWarmup...\")\n",
    "_ = generate_and_time(pipeline_s, TEST_PROMPTS[0], seed=SEED)\n",
    "print(\"✓ Warmup complete\")\n",
    "\n",
    "# Generate test images without caching\n",
    "print(\"\\nGenerating images (no caching)...\")\n",
    "images_s_nocache = []\n",
    "times_s_nocache = []\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"\\nPrompt {i+1}/{len(TEST_PROMPTS)}: {prompt}\")\n",
    "    image, elapsed = generate_and_time(pipeline_s, prompt, seed=SEED+i)\n",
    "    images_s_nocache.append(image)\n",
    "    times_s_nocache.append(elapsed)\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "avg_time_s_nocache = np.mean(times_s_nocache)\n",
    "print(f\"\\nAverage time (S no caching): {avg_time_s_nocache:.2f}s\")\n",
    "\n",
    "results_s['no_cache'] = {\n",
    "    'images': images_s_nocache,\n",
    "    'times': times_s_nocache,\n",
    "    'avg_time': avg_time_s_nocache\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 S - DualCache Aggressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable aggressive caching\n",
    "enable_dualcache(pipeline_s, mode='aggressive')\n",
    "\n",
    "# Warmup with cache\n",
    "print(\"\\nWarmup with cache...\")\n",
    "_ = generate_and_time(pipeline_s, TEST_PROMPTS[0], seed=SEED)\n",
    "print(\"✓ Warmup complete\")\n",
    "\n",
    "# Generate test images\n",
    "print(\"\\nGenerating images (DualCache Aggressive)...\")\n",
    "images_s_aggressive = []\n",
    "times_s_aggressive = []\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"\\nPrompt {i+1}/{len(TEST_PROMPTS)}: {prompt}\")\n",
    "    image, elapsed = generate_and_time(pipeline_s, prompt, seed=SEED+i)\n",
    "    images_s_aggressive.append(image)\n",
    "    times_s_aggressive.append(elapsed)\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "avg_time_s_aggressive = np.mean(times_s_aggressive)\n",
    "speedup_s_aggressive = avg_time_s_nocache / avg_time_s_aggressive\n",
    "print(f\"\\nAverage time (S aggressive): {avg_time_s_aggressive:.2f}s\")\n",
    "print(f\"Speedup: {speedup_s_aggressive:.2f}x\")\n",
    "\n",
    "cache_stats_s_aggressive = cache_dit.summary(pipeline_s)\n",
    "\n",
    "results_s['aggressive'] = {\n",
    "    'images': images_s_aggressive,\n",
    "    'times': times_s_aggressive,\n",
    "    'avg_time': avg_time_s_aggressive,\n",
    "    'speedup': speedup_s_aggressive,\n",
    "    'cache_stats': cache_stats_s_aggressive\n",
    "}\n",
    "\n",
    "disable_cache(pipeline_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 S - DualCache Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable conservative caching\n",
    "enable_dualcache(pipeline_s, mode='conservative')\n",
    "\n",
    "# Warmup with cache\n",
    "print(\"\\nWarmup with cache...\")\n",
    "_ = generate_and_time(pipeline_s, TEST_PROMPTS[0], seed=SEED)\n",
    "print(\"✓ Warmup complete\")\n",
    "\n",
    "# Generate test images\n",
    "print(\"\\nGenerating images (DualCache Conservative)...\")\n",
    "images_s_conservative = []\n",
    "times_s_conservative = []\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"\\nPrompt {i+1}/{len(TEST_PROMPTS)}: {prompt}\")\n",
    "    image, elapsed = generate_and_time(pipeline_s, prompt, seed=SEED+i)\n",
    "    images_s_conservative.append(image)\n",
    "    times_s_conservative.append(elapsed)\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "\n",
    "avg_time_s_conservative = np.mean(times_s_conservative)\n",
    "speedup_s_conservative = avg_time_s_nocache / avg_time_s_conservative\n",
    "print(f\"\\nAverage time (S conservative): {avg_time_s_conservative:.2f}s\")\n",
    "print(f\"Speedup: {speedup_s_conservative:.2f}x\")\n",
    "\n",
    "cache_stats_s_conservative = cache_dit.summary(pipeline_s)\n",
    "\n",
    "results_s['conservative'] = {\n",
    "    'images': images_s_conservative,\n",
    "    'times': times_s_conservative,\n",
    "    'avg_time': avg_time_s_conservative,\n",
    "    'speedup': speedup_s_conservative,\n",
    "    'cache_stats': cache_stats_s_conservative\n",
    "}\n",
    "\n",
    "disable_cache(pipeline_s)\n",
    "del pipeline_s\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n✓ S experiments complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance summary using utility function\n",
    "print_performance_summary(results_xl, results_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XL results using utility function\n",
    "print(\"\\nXL Mode Comparison:\")\n",
    "for i in range(len(TEST_PROMPTS)):\n",
    "    visualize_comparison(results_xl, 'XL', TEST_PROMPTS, OUTPUT_DIR, prompt_idx=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize S results using utility function\n",
    "print(\"\\nS Mode Comparison:\")\n",
    "for i in range(len(TEST_PROMPTS)):\n",
    "    visualize_comparison(results_s, 'S', TEST_PROMPTS, OUTPUT_DIR, prompt_idx=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison charts using utility function\n",
    "create_performance_charts(results_xl, results_s, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Caching provides significant speedups**: Both aggressive and conservative caching strategies provide substantial performance improvements over no caching.\n",
    "\n",
    "2. **Aggressive vs Conservative trade-off**:\n",
    "   - **Aggressive**: Higher speedup, but may have slightly lower quality in very dynamic scenes\n",
    "   - **Conservative**: Balanced speedup with better quality preservation\n",
    "\n",
    "3. **Elastic modes + Caching = Maximum speedup**: Combining a smaller elastic model (S) with aggressive caching provides the best performance with acceptable quality trade-offs.\n",
    "\n",
    "4. **Visual quality**: As seen in the comparisons above, the quality difference between caching strategies is often minimal for most prompts.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- **For production/quality-critical**: Use **XL + Conservative Cache**\n",
    "- **For fast iteration/prototyping**: Use **S + Aggressive Cache**\n",
    "- **For balanced performance**: Use **XL + Aggressive Cache** or **S + Conservative Cache**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try with your own prompts and use cases\n",
    "2. Experiment with custom cache configurations (Fn, Bn, rdt parameters)\n",
    "3. Measure quality metrics on larger datasets\n",
    "4. Combine with other optimizations (quantization, compilation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
